import pandas as pd
import numpy as np
import os as _os, sys as _sys
from typing import Dict, Tuple
import warnings
import hashlib
try:
    import statsmodels.api as sm
    from statsmodels.tools.sm_exceptions import IterationLimitWarning
except Exception:
    raise RuntimeError("éœ€è¦å®‰è£… statsmodels>=0.13 æ‰èƒ½è¿è¡Œ QuantRegï¼ˆpip install statsmodelsï¼‰ã€‚")

# ç¡®ä¿å¯ä»¥ä»¥ç»å¯¹æ–¹å¼å¯¼å…¥ `glm_plus`ï¼ˆæŠŠé¡¹ç›®æ ¹ç›®å½•åŠ å…¥ sys.pathï¼‰
_PROJECT_ROOT = _os.path.abspath(_os.path.join(_os.path.dirname(__file__), '..'))
if _PROJECT_ROOT not in _sys.path:
    _sys.path.insert(0, _PROJECT_ROOT)
from glm_plus.frequentist.torque import FrequentistOQR
from matplotlib import pyplot as plt

# ==================== å¸¸é‡ä¸é…ç½® ====================
# åˆ†ä½ç‚¹é›†åˆï¼ˆåº•/ä¸­/é¡¶ï¼‰ã€‚å¦‚éœ€æ›´ç»†åˆ†ä½ç‚¹ï¼Œå¯æ‰©å±•æ­¤å…ƒç»„
QUANTILES_MAIN = (0.2, 0.5, 0.8)
ORDER_LABELS = ["Assistant/Junior", "Regular", "Leader", "Chief/Founder"]

# ä¼°è®¡å¯¹è±¡ï¼ˆestimandï¼‰ä¸äºŒé˜¶æ®µå¼€å…³
# - 'single_index': å…³é—­ two-indexï¼Œåªåœ¨ h(Y) å°ºå­ä¸ŠæŠ½ç³»æ•°ï¼ˆè§£é‡Šæœ€ç›´è§‚ï¼‰
# - 'final_combo': ä¿æŒ two-indexï¼Œä¸€å¾‹æŠ½â€œæœ€ç»ˆç»„åˆâ€u_tau = XÎ²1 + g^{-1}(XÎ²2,Ï„) çš„çº¿æ€§è¿‘ä¼¼ç³»æ•°
ESTIMAND_MODE = 'single_index'  # å¯æ”¹ä¸º 'final_combo'
# Bootstrap æ¬¡æ•°ï¼ˆ200 å·²è¶³å¤Ÿç¨³å¥ï¼›å¦‚éœ€æ›´ç¨³å¯æ”¹ 500ï¼‰
N_BOOTSTRAP_DEFAULT = 200
# QuantReg å®¹å·®ï¼ˆæé«˜æ”¶æ•›ç¨³å®šæ€§ï¼‰
QR_P_TOL = 1e-6

# â€”â€” æ•ˆåº”é‡é˜ˆå€¼ï¼ˆé¿å…â€œæ˜¾è‘—ä½†æ— æ„ä¹‰â€ï¼‰â€”â€”
# åªæœ‰å½“ |Î”| â‰¥ Îµ ä¸” CI ä¸è·¨ 0 æ‰å°† Î” åˆ¤ä¸º sticky/glass
# Îµ å¯æŒ‰ h(Y) çš„ SD æˆ– IQR çš„æ¯”ä¾‹è®¾å®š
EFFECT_SIZE_MODE = 'sd_pct'   # å¯é€‰: 'sd_pct' æˆ– 'iqr_pct'
EPSILON_SD_FRAC = 0.02        # Îµ = 2% * sd(h(Y))
EPSILON_IQR_FRAC = 0.01       # Îµ = 1% * IQR(h(Y))

# â€”â€” å¯é€‰ï¼šç½®æ¢æ£€éªŒï¼ˆé»˜è®¤å…³é—­ï¼Œè®¾ä¸º >0 å¼€å¯ï¼Œä¾‹å¦‚ 500ï¼‰â€”â€”
N_PERMUTATION_DEFAULT = 0

# â€”â€” æ¦‚ç‡å·®é˜ˆå€¼ï¼ˆå¯è§£é‡Šé‡çº²ï¼‰ï¼šè‡³å°‘ 2 ä¸ªç™¾åˆ†ç‚¹ â€”â€”
PROB_DIFF_THRESHOLD = 0.02

# ç®€å•è¿›åº¦æ—¥å¿—å‡½æ•°ï¼ˆæŒ‰æ­¥éª¤æ‰“å°ï¼Œä¾¿äºåˆå­¦è€…ç†è§£ï¼‰
def _log(msg: str) -> None:
    print(f"[Progress] {msg}")

# subsample è§„åˆ™ï¼šå¤§æ ·æœ¬ä¸‹åŠ é€Ÿæ‹Ÿåˆï¼›ä¸æ”¹å˜ä¼°è®¡å«ä¹‰
SUBSAMPLE_RULE = lambda n: (5000 if n > 20000 else (3000 if n > 3000 else None))

# è¾“å‡ºç›®å½•
import os
OUTPUT_DIR = "output_results"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ç»Ÿä¸€æ‹Ÿåˆå‡½æ•°ï¼ˆä¿æŒä¸ä½ ç°æœ‰ pipeline ä¸€è‡´çš„é…ç½®ï¼‰
def fit_model(X: np.ndarray, y: np.ndarray, taus=QUANTILES_MAIN, random_state: int = 0) -> FrequentistOQR:
    # æœ€å°æ”¹æ³•ï¼šæ¯å›½åªæ‹Ÿåˆä¸€æ¬¡ï¼›å›ºå®šå•æŒ‡æ•°ï¼›è½»é‡ç½‘æ ¼ä»¥æé€Ÿ
    subsample_dynamic = SUBSAMPLE_RULE(X.shape[0]) or 2000
    m = FrequentistOQR(
        quantiles=taus,
        use_two_index=False,
        auto_select_k=True,
        subsample_n=int(subsample_dynamic),
        rank_grid_n=31,
        t_grid_n=41,
        random_state=random_state,
        qr_p_tol=QR_P_TOL,
    )
    m.fit(X, y)
    return m

# ==================== Step 1: è¯»å–æ•°æ® ====================
_log("[1/3] è¯»å–æ•°æ®å¹¶æ¸…æ´—ç±»åˆ«/å“‘å…ƒ...")
_DATA_DIR = _os.path.dirname(__file__)
_CSV_PATH = _os.path.join(_DATA_DIR, "df_seniority.csv")
assert _os.path.exists(_CSV_PATH), f"æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶: {_CSV_PATH}"
df_seniority = pd.read_csv(_CSV_PATH)
assert 'country' in df_seniority.columns, "df_seniority éœ€è¦åŒ…å« 'country' åˆ—"

# â€”â€” ç»Ÿä¸€æ¸…æ´— Y10 æ ‡ç­¾ï¼Œé¿å…å¤§å°å†™/ç©ºæ ¼å¯¼è‡´çš„ä¸¢è¡Œ â€”â€”
_CANON_MAP = {
    "assistant/junior": "Assistant/Junior",
    "regular": "Regular",
    "leader": "Leader",
    "chief/founder": "Chief/Founder",
}

def _canonize_y10(val: str) -> str:
    s = str(val).strip().lower()
    if not s:
        return s
    # å½’ä¸€åŒ–åˆ†éš”ç¬¦ä¸ç©ºæ ¼
    for ch in ["&", "and", "-", "_"]:
        s = s.replace(ch, "/")
    s = "/".join([t.strip() for t in s.split("/") if t.strip()])
    # å…³é”®è¯æ˜ å°„
    if "assistant" in s or "junior" in s:
        key = "assistant/junior"
    elif "regular" in s:
        key = "regular"
    elif "leader" in s:
        key = "leader"
    elif "chief" in s or "founder" in s:
        key = "chief/founder"
    else:
        # æœªè¯†åˆ«åˆ™åŸæ ·è¿”å›ï¼ˆåç»­å°†è¢«ä¸¢å¼ƒï¼Œä¸”æ‰“å°æç¤ºï¼‰
        return str(val).strip()
    return _CANON_MAP[key]

df_seniority['Y10'] = df_seniority['Y10'].apply(_canonize_y10)
unknown_labels = sorted(set(df_seniority['Y10']) - set(ORDER_LABELS))
if unknown_labels:
    print(f"[warn] Y10 ä¸­å­˜åœ¨æœªè¯†åˆ«æ ‡ç­¾ï¼Œå°†è¢«ä¸¢å¼ƒ: {unknown_labels}")
    vc = df_seniority['Y10'].value_counts()
    unk_counts = {lab: int(vc.get(lab, 0)) for lab in unknown_labels}
    print(f"[warn] æœªè¯†åˆ«æ ‡ç­¾çš„æ ·æœ¬é‡: {unk_counts}")
print("[info] å·²çŸ¥æ ‡ç­¾æ ·æœ¬é‡:")
print(df_seniority['Y10'].value_counts().reindex(ORDER_LABELS).fillna(0).astype(int))

# â€”â€” é¢„å»ºç«‹å…¨å±€å“‘å…ƒåˆ—é›†åˆï¼Œç¡®ä¿å„å›½è®¾è®¡çŸ©é˜µåˆ—ä¸€è‡´ â€”â€”
_CAT_COLS = ['highest_educational_degree', 'internationalization', 'company_size']
global_cat_dummies = pd.get_dummies(df_seniority[_CAT_COLS], columns=_CAT_COLS, drop_first=True, dtype=int)
GLOBAL_DUMMY_COLUMNS = list(global_cat_dummies.columns)

_log("[1/3] æ•°æ®åŠ è½½ä¸æ¸…æ´—å®Œæˆ")

# è®¾è®¡çŸ©é˜µæ„é€ ï¼ˆå°ç™½å‹å¥½ï¼šæŠŠ y ç¼–ç ã€female æŒ‡ç¤ºå˜é‡ã€ç±»åˆ«å“‘å…ƒéƒ½åœ¨è¿™é‡Œåšï¼‰
def build_design_for_subset(df_sub: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, np.ndarray, np.ndarray]:
    cols_needed = [
        'gender', 'highest_educational_degree',
        'whether_bachelor_university_prestigious',
        'internationalization', 'work_years', 'company_size', 'Y10'
    ]
    d = df_sub[cols_needed].dropna().copy()
    # y: æŠŠèŒä½ç­‰çº§è½¬ä¸ºæœ‰åºç±»åˆ«ï¼Œå†ç¼–ç ä¸º 1..J
    cat_type = pd.CategoricalDtype(categories=ORDER_LABELS, ordered=True)
    d['y_cat'] = d['Y10'].astype(cat_type)
    d = d[~d['y_cat'].isna()].copy()
    d['y'] = d['y_cat'].cat.codes + 1
    # female: æ–‡æœ¬è½¬å°å†™ååŒ¹é…
    gg = d['gender'].astype(str).str.strip().str.lower()
    d['female'] = (gg == 'female').astype(int)
    # prestigious_bachelor: ç»Ÿä¸€æˆ 0/1
    col_p = 'whether_bachelor_university_prestigious'
    if str(d[col_p].dtype) == 'bool':
        d['prestigious_bachelor'] = d[col_p].astype(int)
    else:
        d['prestigious_bachelor'] = (
            d[col_p].astype(str).str.strip().str.lower().isin(['true','1','yes','y','t'])
        ).astype(int)
    # work_years: è½¬æ•°å€¼
    d['work_years'] = pd.to_numeric(d['work_years'], errors='coerce')
    d = d.dropna(subset=['work_years']).copy()
    # one-hot: ç±»åˆ«å˜é‡åšå“‘å…ƒï¼ˆdrop_first=True ä»¥é¿å…å®Œå…¨å…±çº¿ï¼‰
    cat_cols = _CAT_COLS
    X_cat = pd.get_dummies(d[cat_cols], columns=cat_cols, drop_first=True, dtype=int)
    # ç»Ÿä¸€åˆ°å…¨å±€å“‘å…ƒåˆ—
    X_cat = X_cat.reindex(columns=GLOBAL_DUMMY_COLUMNS, fill_value=0)
    X_df = pd.concat([
        d[['female', 'prestigious_bachelor', 'work_years']].reset_index(drop=True),
        X_cat.reset_index(drop=True)
    ], axis=1)
    X = X_df.to_numpy(dtype=float)
    y = d['y'].to_numpy(dtype=int)
    if d['female'].nunique() < 2:
        try:
            _ctry = str(df_sub.get('country', '').iloc[0]) if 'country' in df_sub.columns else ''
        except Exception:
            _ctry = ''
        print(f"[warn] è¯¥å­é›† female åªæœ‰ä¸€ä¸ªå–å€¼ï¼Œåˆ†ä½ç³»æ•°å¯èƒ½ä¸å¯è¯†åˆ«æˆ–ä¸ç¨³ã€‚country={_ctry}")
    return d, X_df, X, y

# ==================== Step 2: å·¥å…·å‡½æ•°ï¼ˆæŠ½ç³»æ•° + CI + pï¼‰ ====================
def _extract_female_coef_by_tau(model: FrequentistOQR, X_df: pd.DataFrame, X: np.ndarray, taus, mode: str = ESTIMAND_MODE, warn_counter: Dict[str, int] | None = None) -> Dict[float, float]:
    """
    è¿”å› {tau: female ç³»æ•°}ï¼ˆåœ¨æ¨¡å‹å½“å‰æ‹Ÿåˆä¸‹ï¼‰ã€‚
    ä¼˜å…ˆä½¿ç”¨æ¨¡å‹å†…å­˜å‚¨ï¼›å¦åˆ™ç”¨ h(Y) çš„è¿ç»­å“åº”é‡æ–°åš QuantReg å…œåº•ã€‚
    """
    coef_by_tau: Dict[float, float] = {}
    female_idx = int(X_df.columns.get_loc('female'))

    # å…¨å±€å·²å¯¼å…¥ sm ä¸ IterationLimitWarning

    # Single-indexï¼šç›´æ¥åœ¨ h(Y) ä¸ŠæŠ½ç³»æ•°
    if mode == 'single_index':
        # 1) ä¼˜å…ˆå°è¯•ï¼šæ¨¡å‹å†…éƒ¨æ˜¯å¦å·²æš´éœ² per-Ï„ çš„ QR ç»“æœ
        qr_store = getattr(model, 'qr_models_', None) or getattr(model, 'qr_results_', None)
        if isinstance(qr_store, dict) and len(qr_store) > 0:
            for tau in taus:
                res = qr_store.get(tau, None)
                if res is not None and hasattr(res, 'params'):
                    coef_by_tau[float(tau)] = float(np.asarray(res.params, dtype=float)[female_idx])
        # 2) å…œåº•ï¼šä½¿ç”¨ h_iso_ å’Œ _y_jit é‡å»ºè¿ç»­å“åº”ï¼Œå†å¯¹ (hy, X) åš QuantReg
        missing = [t for t in taus if float(t) not in coef_by_tau]
        if missing:
            hy = None
            try:
                hy = model.h_iso_.transform(getattr(model, '_y_jit'))  # type: ignore[attr-defined]
            except Exception:
                hy = None
            if hy is None:
                for name in ('y_tilde_', 'y_cont_', 'y_transformed_', 'y_star_', 'y_continuous_'):
                    hy = getattr(model, name, None)
                    if hy is not None:
                        break
            if hy is None:
                raise RuntimeError("æ— æ³•è·å–æ¨¡å‹çš„è¿ç»­å“åº”ï¼ˆh(Y)ï¼‰ã€‚å¯ç”¨ dir(model) æŸ¥çœ‹å®é™…å­—æ®µåååŠ å…¥å€™é€‰åˆ—è¡¨ã€‚")
            hy = np.asarray(hy, dtype=float).reshape(-1)
            X_used = np.asarray(X, dtype=float)
            if X_used.shape[0] != hy.shape[0]:
                X_last = getattr(model, '_last_X_', None)
                if X_last is None or np.asarray(X_last).shape[0] != hy.shape[0]:
                    raise RuntimeError("X ä¸è¿ç»­å“åº”é•¿åº¦ä¸åŒ¹é…ï¼Œä¸” model._last_X_ ä¸å¯ç”¨ã€‚")
                X_used = np.asarray(X_last, dtype=float)
            for tau in missing:
                with warnings.catch_warnings(record=True) as w:
                    warnings.simplefilter("always", IterationLimitWarning)
                    try:
                        res = sm.QuantReg(hy, X_used).fit(q=float(tau), max_iter=5000, p_tol=QR_P_TOL)
                    except TypeError:
                        res = sm.QuantReg(hy, X_used).fit(q=float(tau), max_iter=5000)
                if warn_counter is not None:
                    warn_counter['fit_count'] = warn_counter.get('fit_count', 0) + 1
                    warn_counter['warn_count'] = warn_counter.get('warn_count', 0) + sum(1 for wi in w if issubclass(wi.category, IterationLimitWarning))
                coef_by_tau[float(tau)] = float(np.asarray(res.params, dtype=float)[female_idx])
        return coef_by_tau

    # final_comboï¼šå¯¹ u_tau = XÎ²1 + g^{-1}(XÎ²2,Ï„) åšä¸€é Ï„-QuantRegï¼ŒæŠ½ female ç³»æ•°
    X_used = np.asarray(X, dtype=float)
    X_last = getattr(model, '_last_X_', None)
    if X_last is not None and X_used.shape[0] != np.asarray(X_last).shape[0]:
        X_used = np.asarray(X_last, dtype=float)
    xb1 = X_used @ (model.beta1_.reshape(-1) if model.beta1_ is not None else np.zeros(X_used.shape[1]))
    for tau in taus:
        u_tau = xb1.copy()
        if getattr(model, 'use_two_index', False) and getattr(model, 'beta2_tau_', None) is not None:
            b2t = model.beta2_tau_.get(float(tau), model.beta2_)
            if b2t is None and isinstance(model.beta2_tau_, dict) and len(model.beta2_tau_) > 0:
                b2t = next(iter(model.beta2_tau_.values()))
            if b2t is not None:
                u2 = (X_used @ b2t.reshape(-1))
                try:
                    r_tau = model._ginv(u2)
                except Exception:
                    r_tau = u2
                u_tau = u_tau + r_tau
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter("always", IterationLimitWarning)
            try:
                res = sm.QuantReg(u_tau, X_used).fit(q=float(tau), max_iter=5000, p_tol=QR_P_TOL)
            except TypeError:
                res = sm.QuantReg(u_tau, X_used).fit(q=float(tau), max_iter=5000)
        if warn_counter is not None:
            warn_counter['fit_count'] = warn_counter.get('fit_count', 0) + 1
            warn_counter['warn_count'] = warn_counter.get('warn_count', 0) + sum(1 for wi in w if issubclass(wi.category, IterationLimitWarning))
        coef_by_tau[float(tau)] = float(np.asarray(res.params, dtype=float)[female_idx])

    return coef_by_tau

def _ci_from_samples(samples, level=0.95):
    arr = np.asarray(samples, dtype=float)
    arr = arr[np.isfinite(arr)]
    if arr.size == 0:
        return np.nan, np.nan, False
    lo = float(np.nanpercentile(arr, (1.0 - level) / 2.0 * 100.0))
    hi = float(np.nanpercentile(arr, (1.0 + level) / 2.0 * 100.0))
    sig = not (lo <= 0.0 <= hi)
    return lo, hi, sig

def _p_from_sign_two_sided(samples):
    arr = np.asarray(samples, dtype=float)
    return float(2.0 * min(np.mean(arr >= 0.0), np.mean(arr <= 0.0)))

# ==================== Step 3: æŒ‰å›½å®¶æå–â€œfemale åˆ†ä½ç³»æ•° + CIâ€ ====================
def analyze_by_country_detailed(min_n: int = 500, n_bootstrap: int = N_BOOTSTRAP_DEFAULT, random_state: int = 0) -> Tuple[pd.DataFrame, pd.DataFrame]:
    female_coef_rows = []            # ç‚¹ä¼°
    female_coef_boot_rows = []       # Bootstrap CI
    delta_rows = []                  # å›½å®¶çº§ Î” å…œåº•æ±‡æ€»

    print("=== æŒ‰å›½å®¶æŠ½å– being female çš„åˆ†ä½ç³»æ•°ï¼ˆå«95% bootstrap CIï¼‰===")
    print("[note] æœ¬åˆ†æçŸ©é˜µä¸å«å¸¸æ•°åˆ—ï¼ˆæ‹¦æˆªé¡¹ï¼‰ï¼Œfemale ç³»æ•°ä¸ºç›¸å¯¹æ— æˆªè·çº¿æ€§é¡¹çš„è¾¹é™…æ•ˆåº”ã€‚")
    if ESTIMAND_MODE == 'final_combo':
        print("[note] final_combo æ¨¡å¼ï¼šå¯¹æœ€ç»ˆç»„åˆ u_Ï„ è¿›è¡Œçº¿æ€§æŠ•å½±ï¼ˆQuantRegï¼‰ï¼Œç³»æ•°ä¸ºçº¿æ€§è¿‘ä¼¼ï¼Œä¸ä½œç»“æ„æ€§è§£é‡Šã€‚")
    for ctry, df_g in df_seniority.groupby('country'):
        d, Xdf, Xg, yg = build_design_for_subset(df_g)
        n = len(yg)
        if n < min_n:
            print(f"{ctry}: è·³è¿‡ (n={n} < {min_n})")
            continue
        print(f"{ctry}: æ‹Ÿåˆæ¨¡å‹å¹¶è®°å½•ç‚¹ä¼° (n={n})â€¦")

        # 1) æ‹Ÿåˆä¸»æ¨¡å‹å¹¶è®°å½•ç‚¹ä¼°æ›²çº¿
        m = fit_model(Xg, yg, taus=QUANTILES_MAIN, random_state=int(random_state))
        warn_counter = {'fit_count': 0, 'warn_count': 0}
        try:
            coef_point = _extract_female_coef_by_tau(m, Xdf, Xg, QUANTILES_MAIN, mode=ESTIMAND_MODE, warn_counter=warn_counter)
            for tau, val in coef_point.items():
                female_coef_rows.append({
                    'country': str(ctry), 'n': n, 'tau': float(tau),
                    'female_coef_point': float(val)
                })
        except Exception as e:
            print(f"[warn] æ— æ³•æŠ½å–ç³»æ•°ç‚¹ä¼°ï¼ˆ{ctry}ï¼‰ï¼š{e}")

        # 2) Bootstrapï¼šå›ºå®š h/gï¼Œä»…å¯¹ QR æŠ½æ ·ç³»æ•°ï¼ˆä¸€æ¬¡æ‹Ÿåˆ + æŠ½æ ·ï¼‰
        print(f"{ctry}: è¿›è¡Œ bootstrapï¼ˆ{int(n_bootstrap)} æ¬¡ï¼Œå›ºå®š h/gï¼Œä»… QR é‡ä¼°ï¼‰ä»¥æ„é€  CIâ€¦")
        seed = int(hashlib.md5(str(ctry).encode()).hexdigest()[:8], 16) + int(random_state)
        TAUS_FOR_BOOT = (min(QUANTILES_MAIN), max(QUANTILES_MAIN))
        q_low, q_high = TAUS_FOR_BOOT
        female_idx = int(Xdf.columns.get_loc('female'))
        boot = m.bootstrap_inference(n_boot=int(n_bootstrap), random_state=seed, return_coefs=True)
        # å¸¸è§è¿”å›ï¼šbeta_tau[tau]['draws'] -> (n_boot, n_params)
        coef_boot = {tau: [] for tau in TAUS_FOR_BOOT}
        delta_boot = []
        for tau in TAUS_FOR_BOOT:
            draws = boot.get('beta_tau', {}).get(float(tau), {}).get('draws', None)
            if draws is None:
                continue
            coef_boot[tau] = list(np.asarray(draws)[:, female_idx].reshape(-1))
        if (q_low in coef_boot) and (q_high in coef_boot) and len(coef_boot[q_low]) > 0 and len(coef_boot[q_high]) > 0:
            a = np.asarray(coef_boot[q_high], dtype=float)
            b = np.asarray(coef_boot[q_low], dtype=float)
            n_success = int(min(a.shape[0], b.shape[0]))
            delta_boot = list((a[:n_success] - b[:n_success]).reshape(-1))
        else:
            n_success = 0

        # 2.1) æ„é€ è¿ç»­å“åº” h(Y)ï¼Œç”¨äºæ•ˆåº”é‡é˜ˆå€¼è®¡ç®—ä¸å¯é€‰ç½®æ¢æ£€éªŒ
        hy = None
        try:
            hy = m.h_iso_.transform(getattr(m, '_y_jit'))  # type: ignore[attr-defined]
        except Exception:
            hy = None
        if hy is None:
            for name in ('y_tilde_', 'y_cont_', 'y_transformed_', 'y_star_', 'y_continuous_'):
                hy = getattr(m, name, None)
                if hy is not None:
                    break
        if hy is None:
            # è‹¥æ— æ³•ç›´æ¥è·å– h(Y)ï¼Œç”¨ X @ beta1_ è¿‘ä¼¼ï¼ˆä»…ç”¨äºå®šä¹‰å°ºåº¦ï¼Œä¸ä½œæ¨æ–­ï¼‰
            try:
                hy = (np.asarray(Xg, dtype=float) @ (m.beta1_.reshape(-1) if m.beta1_ is not None else np.zeros(Xg.shape[1])))
            except Exception:
                hy = np.asarray([np.nan] * Xg.shape[0])
        hy = np.asarray(hy, dtype=float).reshape(-1)
        hy_sd = float(np.nanstd(hy, ddof=1)) if np.isfinite(hy).all() else np.nan
        if np.isfinite(hy).all():
            hy_q25 = float(np.nanpercentile(hy, 25.0))
            hy_q75 = float(np.nanpercentile(hy, 75.0))
            hy_iqr = float(hy_q75 - hy_q25)
        else:
            hy_q25, hy_q75, hy_iqr = np.nan, np.nan, np.nan
        if EFFECT_SIZE_MODE == 'sd_pct' and np.isfinite(hy_sd):
            epsilon = float(EPSILON_SD_FRAC * hy_sd)
            epsilon_mode = 'sd_pct'
        elif EFFECT_SIZE_MODE == 'iqr_pct' and np.isfinite(hy_iqr):
            epsilon = float(EPSILON_IQR_FRAC * hy_iqr)
            epsilon_mode = 'iqr_pct'
        else:
            epsilon = np.nan
            epsilon_mode = 'undefined'

        # 3) CI ä¸æ˜¾è‘—æ€§
        bottom_sig_neg = False
        top_sig_neg = False
        bottom_sig_pos = False
        top_sig_pos = False
        for tau in QUANTILES_MAIN:
            samples = coef_boot.get(tau, [])
            if samples:
                lo, hi, sig = _ci_from_samples(samples, level=0.95)
                p_boot = _p_from_sign_two_sided(samples)
                # æ‰¾å¯¹åº”ç‚¹ä¼°
                pt = np.nan
                try:
                    pt = float([r['female_coef_point'] for r in female_coef_rows if (r['country'] == str(ctry) and abs(r['tau'] - float(tau)) < 1e-9)][-1])
                except Exception:
                    pass
                female_coef_boot_rows.append({
                    'country': str(ctry), 'n': n, 'tau': float(tau),
                    'female_coef_point': pt,
                    'female_coef_ci_low': lo, 'female_coef_ci_high': hi,
                    'female_coef_sig': sig, 'female_coef_p_boot': float(p_boot),
                    'ci_level': 0.95
                })
                if abs(float(tau) - float(q_low)) < 1e-9:
                    bottom_sig_neg = (hi < 0.0)
                    bottom_sig_pos = (lo > 0.0)
                if abs(float(tau) - float(q_high)) < 1e-9:
                    top_sig_neg = (hi < 0.0)
                    top_sig_pos = (lo > 0.0)

        # è‹¥ female åˆ—ä¸å­˜åœ¨æˆ–å…¨å¸¸æ•°ï¼Œè·³è¿‡åç»­ Î” åˆ¤å®š
        if 'female' not in Xdf.columns or Xdf['female'].nunique() < 2:
            print(f"[warn] è·³è¿‡ Î” åˆ¤å®šï¼š{ctry} çš„ female åˆ—ä¸å­˜åœ¨æˆ–æ— å˜å¼‚")
            continue

        # 3.1) ç›´æ¥æ£€éªŒ Î”_top-bottom çš„æ˜¾è‘—æ€§ï¼ˆå åŠ æ•ˆåº”é‡é˜ˆå€¼ Îµï¼‰
        delta_row = None
        if delta_boot:
            d_lo, d_hi, d_sig = _ci_from_samples(delta_boot, level=0.95)
            d_p = _p_from_sign_two_sided(delta_boot)
            d_point = np.nan
            try:
                pt_top = float([r['female_coef_point'] for r in female_coef_rows if (r['country'] == str(ctry) and abs(r['tau'] - float(q_high)) < 1e-9)][-1])
                pt_bot = float([r['female_coef_point'] for r in female_coef_rows if (r['country'] == str(ctry) and abs(r['tau'] - float(q_low)) < 1e-9)][-1])
                d_point = pt_top - pt_bot
            except Exception:
                pass
            delta_abs = float(abs(d_point)) if np.isfinite(d_point) else np.nan
            delta_std_sd = float(d_point / hy_sd) if (np.isfinite(d_point) and np.isfinite(hy_sd) and hy_sd > 0) else np.nan
            # æ¦‚ç‡å°ºåº¦ Î”_probï¼ˆåŸºäºä»£è¡¨æ€§æ ·æœ¬ã€Ï„ ç½‘æ ¼è¿‘ä¼¼ï¼‰ï¼š
            # Î”_prob = [P_bottom(f=1)-P_bottom(f=0)] - [P_top(f=1)-P_top(f=0)]
            delta_prob = np.nan
            prob_bottom_f0 = np.nan
            prob_bottom_f1 = np.nan
            prob_top_f0 = np.nan
            prob_top_f1 = np.nan
            try:
                # ä»£è¡¨æ€§æ ·æœ¬ï¼šä½¿ç”¨å„ç‰¹å¾çš„æ ·æœ¬å‡å€¼ï¼›ä»…åˆ‡æ¢ female=0/1
                x_rep = np.asarray(Xdf.mean(), dtype=float).reshape(-1)
                x0 = x_rep.copy(); x1 = x_rep.copy()
                x0[female_idx] = 0.0
                x1[female_idx] = 1.0
                # Ï„ ç½‘æ ¼
                tau_grid = np.linspace(0.02, 0.98, 41)
                y0_list = []
                y1_list = []
                rq_model = sm.QuantReg(hy, Xg)
                for t in tau_grid:
                    try:
                        rq = rq_model.fit(q=float(t), max_iter=5000, p_tol=QR_P_TOL)
                    except TypeError:
                        rq = rq_model.fit(q=float(t), max_iter=5000)
                    b = np.asarray(rq.params, dtype=float).reshape(-1)
                    hy0 = float(x0 @ b); hy1 = float(x1 @ b)
                    y0 = float(m._hinv(np.array([hy0]))[0])
                    y1 = float(m._hinv(np.array([hy1]))[0])
                    y0_list.append(y0)
                    y1_list.append(y1)
                y0_arr = np.asarray(y0_list, dtype=float)
                y1_arr = np.asarray(y1_list, dtype=float)
                # ç±»åˆ«è¾¹ç•Œ
                j_min = int(m._y_min_cat) if m._y_min_cat is not None else int(np.nanmin(yg))
                j_max = int(m._y_max_cat) if m._y_max_cat is not None else int(np.nanmax(yg))
                # åº•éƒ¨æ¦‚ç‡: P(Y<=j_min)
                mask0_bottom = (y0_arr <= float(j_min) + 1e-9)
                mask1_bottom = (y1_arr <= float(j_min) + 1e-9)
                prob_bottom_f0 = float(np.max(tau_grid[mask0_bottom])) if np.any(mask0_bottom) else 0.0
                prob_bottom_f1 = float(np.max(tau_grid[mask1_bottom])) if np.any(mask1_bottom) else 0.0
                # é¡¶éƒ¨æ¦‚ç‡: P(Y=j_max) = 1 - P(Y<=j_max-1)
                thresh_top = float(j_max - 1)
                mask0_topcdf = (y0_arr <= thresh_top + 1e-9)
                mask1_topcdf = (y1_arr <= thresh_top + 1e-9)
                F_topminus1_f0 = float(np.max(tau_grid[mask0_topcdf])) if np.any(mask0_topcdf) else 0.0
                F_topminus1_f1 = float(np.max(tau_grid[mask1_topcdf])) if np.any(mask1_topcdf) else 0.0
                prob_top_f0 = float(1.0 - F_topminus1_f0)
                prob_top_f1 = float(1.0 - F_topminus1_f1)
                delta_prob = (prob_bottom_f1 - prob_bottom_f0) - (prob_top_f1 - prob_top_f0)
            except Exception:
                delta_prob = np.nan

            # åˆ†ç±»è§„åˆ™ï¼ˆå«è¾¹è§’ï¼‰
            cls = 'no_heterogeneity'
            reason = 'è§„åˆ™æœªè§¦å‘æ˜¾è‘—å·®å¼‚'
            above_eps = (np.isfinite(delta_abs) and np.isfinite(epsilon) and delta_abs >= epsilon)
            meets_prob = (np.isfinite(delta_prob) and abs(float(delta_prob)) >= float(PROB_DIFF_THRESHOLD))
            if d_sig and above_eps and meets_prob and d_point > 0 and (delta_prob > 0):
                cls = 'sticky_floor'; reason = f'Î”>0ã€æ˜¾è‘—ï¼Œ|Î”|â‰¥Îµï¼ˆ{epsilon_mode}ï¼‰ï¼Œä¸” Î”_probâ‰¥{PROB_DIFF_THRESHOLD:.2f}'
            elif d_sig and above_eps and meets_prob and d_point < 0 and (delta_prob < 0):
                cls = 'glass_ceiling'; reason = f'Î”<0ã€æ˜¾è‘—ï¼Œ|Î”|â‰¥Îµï¼ˆ{epsilon_mode}ï¼‰ï¼Œä¸” Î”_probâ‰¥{PROB_DIFF_THRESHOLD:.2f}'
            elif d_sig and (not above_eps):
                cls = 'no_heterogeneity_small_effect'; reason = 'Î” æ˜¾è‘—ä½†æ•ˆåº”é‡ä½äºé˜ˆå€¼ Îµ'
            elif d_sig and above_eps and (not meets_prob):
                cls = 'no_heterogeneity_small_prob_effect'; reason = f'Î” æ˜¾è‘—ä¸” |Î”|â‰¥Îµï¼Œä½† Î”_prob<{PROB_DIFF_THRESHOLD:.2f}'
            elif bottom_sig_neg and top_sig_neg and not d_sig:
                cls = 'double_disadvantage'; reason = 'ä¸¤ç«¯å‡æ˜¾è‘—<0ï¼Œä½†åˆ†ä½å·®Î”ä¸æ˜¾è‘—'
            elif bottom_sig_pos and not (top_sig_neg or top_sig_pos):
                cls = 'female_advantage_bottom'; reason = 'åº•éƒ¨æ˜¾è‘—>0ï¼Œé¡¶éƒ¨ä¸æ˜¾è‘—'
            elif top_sig_pos and not (bottom_sig_neg or bottom_sig_pos):
                cls = 'female_advantage_top'; reason = 'é¡¶éƒ¨æ˜¾è‘—>0ï¼Œåº•éƒ¨ä¸æ˜¾è‘—'
            elif (not bottom_sig_neg and not bottom_sig_pos) and (not top_sig_neg and not top_sig_pos):
                cls = 'both_non_sig'; reason = 'ä¸¤ç«¯å‡ä¸æ˜¾è‘—'

            unstable_flag = (n_success / max(int(n_bootstrap), 1) < 0.6)
            warn_rate = (float(warn_counter.get('warn_count', 0)) / float(warn_counter.get('fit_count', 1)))
            high_warn = bool(warn_rate > 0.2)
            # å¯é€‰ï¼šç½®æ¢æ£€éªŒï¼ˆåœ¨ h(Y) ä¸Šå¿«é€Ÿè¿‘ä¼¼ï¼‰ï¼Œé»˜è®¤å…³é—­
            p_perm = np.nan
            if int(N_PERMUTATION_DEFAULT) > 0 and np.isfinite(d_point):
                try:
                    rng = np.random.default_rng(seed)
                    def _coef_at_tau(hy_arr, X_arr, tau):
                        try:
                            res = sm.QuantReg(hy_arr, X_arr).fit(q=float(tau), max_iter=5000, p_tol=QR_P_TOL)
                        except TypeError:
                            res = sm.QuantReg(hy_arr, X_arr).fit(q=float(tau), max_iter=5000)
                        return float(np.asarray(res.params, dtype=float)[female_idx])
                    def _delta_once(X_arr):
                        c_low = _coef_at_tau(hy, X_arr, q_low)
                        c_high = _coef_at_tau(hy, X_arr, q_high)
                        return float(c_high - c_low)
                    obs_abs = abs(float(d_point))
                    extreme = 0
                    for _ in range(int(N_PERMUTATION_DEFAULT)):
                        X_perm = np.asarray(Xg, dtype=float).copy()
                        X_perm[:, female_idx] = rng.permutation(X_perm[:, female_idx])
                        d_perm = _delta_once(X_perm)
                        if abs(d_perm) >= obs_abs:
                            extreme += 1
                    p_perm = float((extreme + 1) / (int(N_PERMUTATION_DEFAULT) + 1))
                except Exception:
                    p_perm = np.nan

            delta_row = {
                'country': str(ctry), 'n': n,
                'delta_top_bottom_point': d_point,
                'delta_top_bottom_ci_low': d_lo,
                'delta_top_bottom_ci_high': d_hi,
                'delta_top_bottom_sig': d_sig,
                'delta_top_bottom_p_boot': float(d_p),
                'delta_top_bottom_p_perm': p_perm,
                'classification': cls,
                'reason': reason,
                'delta_abs': delta_abs,
                'delta_std_sd': delta_std_sd,
                'hy_sd': hy_sd,
                'hy_iqr': hy_iqr,
                'epsilon': epsilon,
                'epsilon_mode': epsilon_mode,
                'prob_bottom_female0': prob_bottom_f0,
                'prob_bottom_female1': prob_bottom_f1,
                'prob_top_female0': prob_top_f0,
                'prob_top_female1': prob_top_f1,
                'delta_prob': delta_prob,
                'bootstrap_success_n': int(n_success),
                'bootstrap_total_n': int(n_bootstrap),
                'unstable_CI': bool(unstable_flag),
                'qr_warn_count': int(warn_counter.get('warn_count', 0)),
                'qr_fit_count': int(warn_counter.get('fit_count', 0)),
                'qr_warn_rate': warn_rate,
                'high_warn': high_warn,
                'estimand_mode': ESTIMAND_MODE,
            }
            # å°† Î” ä¿¡æ¯é‡å¤é™„åŠ åˆ°æ¯ä¸ª Ï„ çš„è¡Œï¼Œä¾¿äºç»Ÿä¸€å¯¼å‡ºä¸ç­›é€‰
            for r in [row for row in female_coef_boot_rows if row['country'] == str(ctry) and row['n'] == n]:
                r.update({k: v for k, v in delta_row.items() if k not in r})
            # å›½å®¶çº§ Î” è¡Œæ”¶é›†
            delta_rows.append(delta_row)
        else:
            warn_rate = (float(warn_counter.get('warn_count', 0)) / float(warn_counter.get('fit_count', 1)))
            delta_row = {
                'country': str(ctry), 'n': n,
                'delta_top_bottom_point': np.nan,
                'delta_top_bottom_ci_low': np.nan,
                'delta_top_bottom_ci_high': np.nan,
                'delta_top_bottom_sig': False,
                'delta_top_bottom_p_boot': np.nan,
                'classification': 'insufficient_bootstrap',
                'reason': 'Î” æ— æœ‰æ•ˆè‡ªåŠ©æ ·æœ¬',
                'bootstrap_success_n': int(n_success),
                'bootstrap_total_n': int(n_bootstrap),
                'unstable_CI': True,
                'qr_warn_count': int(warn_counter.get('warn_count', 0)),
                'qr_fit_count': int(warn_counter.get('fit_count', 0)),
                'qr_warn_rate': warn_rate,
                'high_warn': True,
                'estimand_mode': ESTIMAND_MODE,
            }
            for r in [row for row in female_coef_boot_rows if row['country'] == str(ctry) and row['n'] == n]:
                r.update({k: v for k, v in delta_row.items() if k not in r})
            # å›½å®¶çº§ Î” è¡Œæ”¶é›†
            delta_rows.append(delta_row)

    female_coef_df = pd.DataFrame(female_coef_rows)
    female_coef_ci_df = pd.DataFrame(female_coef_boot_rows)
    female_coef_all = (
        female_coef_df.merge(
            female_coef_ci_df,
            on=['country', 'n', 'tau'],
            how='outer'
        ).sort_values(['country', 'tau'])
    )
    # ç»Ÿä¸€åˆ—åï¼šåˆå¹¶åå¯èƒ½å‡ºç° female_coef_point_x / female_coef_point_y
    if 'female_coef_point_x' in female_coef_all.columns or 'female_coef_point_y' in female_coef_all.columns:
        # ä¼˜å…ˆä½¿ç”¨ç‚¹ä¼°æ¥æºï¼ˆ_xï¼‰ï¼Œç¼ºå¤±æ—¶ç”¨ CI è¡¨ä¸­çš„å›å¡«ï¼ˆ_yï¼‰
        female_coef_all['female_coef_point'] = female_coef_all.get('female_coef_point_x')
        if 'female_coef_point_y' in female_coef_all.columns:
            female_coef_all['female_coef_point'] = female_coef_all['female_coef_point'].fillna(
                female_coef_all['female_coef_point_y']
            )
        drop_cols = [c for c in ['female_coef_point_x', 'female_coef_point_y'] if c in female_coef_all.columns]
        female_coef_all = female_coef_all.drop(columns=drop_cols)
    return female_coef_all, pd.DataFrame(delta_rows)

_log(f"[2/3] å¼€å§‹åˆ†å›½å®¶ç³»æ•°åˆ†æ (min_n=500, bootstrap={N_BOOTSTRAP_DEFAULT})â€¦")
female_coef_all, delta_df = analyze_by_country_detailed(min_n=500, n_bootstrap=N_BOOTSTRAP_DEFAULT)

print(f"\nå®Œæˆï¼æ¶µç›– {female_coef_all['country'].nunique() if len(female_coef_all) > 0 else 0} ä¸ªå›½å®¶")
print("\n=== being female çš„åˆ†ä½ç³»æ•°ï¼ˆå‰10è¡Œï¼‰===")
print(female_coef_all.head(10))

# ==================== Step 4: å¯¼å‡ºä¸å¯è§†åŒ– ====================
_log("[3/3] å¯¼å‡º CSV å¹¶ç»˜åˆ¶ç³»æ•°â€”åˆ†ä½ç‚¹æ›²çº¿å›¾â€¦")
if len(female_coef_all) > 0:
    out_csv = f'{OUTPUT_DIR}/female_coef_by_quantile.csv'
    female_coef_all.to_csv(out_csv, index=False)
    print(f"âœ“ åˆ†ä½ç³»æ•°è¡¨ï¼ˆfemale + CIï¼‰å·²ä¿å­˜: {out_csv}")
    # è¾“å‡ºå›½å®¶çº§æ±‡æ€»ï¼ˆÎ” ä¸åˆ†ç±» + è¯Šæ–­ï¼‰ï¼Œç›´æ¥ç”¨ delta_dfï¼Œç¡®ä¿æ—  CI å›½å®¶ä¹Ÿæœ‰åˆ†ç±»
    keep_cols = ['country', 'n',
                 'delta_top_bottom_point', 'delta_abs', 'delta_std_sd', 'delta_prob',
                 'prob_bottom_female0', 'prob_bottom_female1', 'prob_top_female0', 'prob_top_female1',
                 'delta_top_bottom_ci_low', 'delta_top_bottom_ci_high',
                 'delta_top_bottom_sig', 'delta_top_bottom_p_boot', 'delta_top_bottom_p_perm',
                 'classification', 'reason',
                 'hy_sd', 'hy_iqr', 'epsilon', 'epsilon_mode',
                 'bootstrap_success_n', 'bootstrap_total_n', 'unstable_CI',
                 'qr_warn_count', 'qr_fit_count', 'qr_warn_rate', 'high_warn',
                 'estimand_mode']
    summary_df = delta_df.reindex(columns=[c for c in keep_cols if c in delta_df.columns])
    out_csv2 = f'{OUTPUT_DIR}/female_coef_summary_by_country.csv'
    summary_df.to_csv(out_csv2, index=False)
    print(f"âœ“ å›½å®¶çº§æ±‡æ€»ï¼ˆÎ” ä¸åˆ†ç±»ï¼‰å·²ä¿å­˜: {out_csv2}")
    print("[note] é™„æ³¨ï¼šqr_warn_rate > 0.2 è§†ä¸ºä¸ç¨³å®šï¼ˆhigh_warn=Trueï¼‰ã€‚")

    # ç”»æ¯å›½ä¸€æ¡æ›²çº¿ï¼Œå¹¶ç”¨é˜´å½±æ ‡å‡º 95% CI
    plt.figure(figsize=(7, 5))
    for ctry, g in female_coef_all.groupby('country'):
        g = g.sort_values('tau')
        plt.plot(g['tau'], g['female_coef_point'], label=str(ctry))
        if {'female_coef_ci_low', 'female_coef_ci_high'}.issubset(g.columns):
            mask = g[['female_coef_ci_low','female_coef_ci_high']].notna().all(axis=1)
            if mask.any():
                plt.fill_between(g.loc[mask, 'tau'], g.loc[mask, 'female_coef_ci_low'], g.loc[mask, 'female_coef_ci_high'], alpha=0.15)
    plt.axhline(0.0, linestyle='--', linewidth=1)
    plt.xlabel('Quantile (Ï„)')
    ylabel = 'Coef of being female (on h(Y) scale)' if ESTIMAND_MODE=='single_index' else 'Linearized coef of being female on final combo u_Ï„'
    plt.ylabel(ylabel)
    # å–ä¸ bootstrap åŒ¹é…çš„åº•/é¡¶åˆ†ä½ç”¨äºæ ‡é¢˜è¯´æ˜
    q_low, q_high = (min(QUANTILES_MAIN), max(QUANTILES_MAIN))
    plt.title(f'Female effect across quantiles (95% CI at Ï„={q_low}, {q_high})')
    plt.legend(fontsize=8)
    plt.tight_layout()
    fig_path = f'{OUTPUT_DIR}/female_coef_curve.png'
    plt.savefig(fig_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"âœ“ ç³»æ•°æ›²çº¿å›¾å·²ä¿å­˜: {fig_path}")
else:
    print("æ— å¯å¯¼å‡ºçš„ç³»æ•°ç»“æœã€‚")

# ç®€æ˜é˜…è¯»å£å¾„ï¼ˆç”¨äºè®ºæ–‡/æŠ¥å‘Šï¼‰
print("\nâ€”â€” å¦‚ä½•åˆ¤è¯»ï¼š")
print("- åº•éƒ¨ Ï„=0.2 æ˜¾è‘—<0ã€é¡¶éƒ¨ Ï„=0.8 â‰ˆ0 â†’ sticky floor")
print("- é¡¶éƒ¨ Ï„=0.8 æ˜¾è‘—<0ã€åº•éƒ¨ Ï„=0.2 â‰ˆ0 â†’ glass ceiling")
print("- ä¸¤ç«¯å‡æ˜¾è‘—<0 â†’ double disadvantageï¼›åŒºé—´è¦†ç›–0 â†’ æ— æ˜¾è‘—åˆ†ä½å¼‚è´¨æ€§")

print("\nğŸ¯ æ ¸å¿ƒæ–¹æ³•æç¤ºï¼šæœ¬è„šæœ¬ç›´æ¥ç”¨ â€œbeing female çš„åˆ†ä½ç³»æ•° + bootstrap CIâ€ éªŒè¯ sticky floor / glass ceilingã€‚" )
print("- è®¾è®¡çŸ©é˜µæ— æˆªè·ï¼šç³»æ•°ä¸ºç›¸å¯¹æ— æˆªè·åŸºå‡†çš„è¾¹é™…æ•ˆåº”ï¼›Î” åˆ¤å®šä¸å—å½±å“ï¼Œä»…éœ€è¯´æ˜ã€‚")
print("- æ¦‚ç‡å°ºåº¦ä¸ºè¿‘ä¼¼ï¼šé€šè¿‡ Ï„â†’h(Y)â†’h^{-1}(Â·)â†’ç±»åˆ«é˜ˆå€¼ çš„ç¦»æ•£è¿‘ä¼¼å¾—åˆ°åº•/é¡¶æ¦‚ç‡å·® Î”_probï¼Œä¾¿äºå¯è§£é‡Šæ€§ã€‚")

